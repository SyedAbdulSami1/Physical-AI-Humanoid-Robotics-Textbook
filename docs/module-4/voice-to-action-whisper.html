<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-module-4/voice-to-action-whisper" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Vision-Language-Action: Voice-to-Action with Speech Recognition | Physical AI &amp; Humanoid Robotics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://SyedAbdulSami1.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://SyedAbdulSami1.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://SyedAbdulSami1.github.io/docs/module-4/voice-to-action-whisper"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Vision-Language-Action: Voice-to-Action with Speech Recognition | Physical AI &amp; Humanoid Robotics"><meta data-rh="true" name="description" content="We&#x27;ve reached the final and most exciting frontier of our curriculum: Vision-Language-Action (VLA) models. This is where we fuse the power of large-scale AI models with the physical embodiment of our robot. The goal is to create a robot that can understand natural human instructions, perceive its environment, and take meaningful action‚Äîa true cognitive robot."><meta data-rh="true" property="og:description" content="We&#x27;ve reached the final and most exciting frontier of our curriculum: Vision-Language-Action (VLA) models. This is where we fuse the power of large-scale AI models with the physical embodiment of our robot. The goal is to create a robot that can understand natural human instructions, perceive its environment, and take meaningful action‚Äîa true cognitive robot."><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://SyedAbdulSami1.github.io/docs/module-4/voice-to-action-whisper"><link data-rh="true" rel="alternate" href="https://SyedAbdulSami1.github.io/docs/module-4/voice-to-action-whisper" hreflang="en"><link data-rh="true" rel="alternate" href="https://SyedAbdulSami1.github.io/docs/module-4/voice-to-action-whisper" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Module 4: Vision-Language-Action (VLA)","item":"https://SyedAbdulSami1.github.io/docs/category/module-4-vision-language-action-vla"},{"@type":"ListItem","position":2,"name":"Vision-Language-Action: Voice-to-Action with Speech Recognition","item":"https://SyedAbdulSami1.github.io/docs/module-4/voice-to-action-whisper"}]}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Physical AI &amp; Humanoid Robotics RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Physical AI &amp; Humanoid Robotics Atom Feed"><link rel="stylesheet" href="/assets/css/styles.8ff61272.css">
<script src="/assets/js/runtime~main.d1cc514a.js" defer="defer"></script>
<script src="/assets/js/main.b7da805c.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Physical AI &amp; Humanoid Robotics Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="Physical AI &amp; Humanoid Robotics Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Textbook</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/gemini-cli/Physical-AI-Humanoid-Robotics-Textbook" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro"><span title="Course Overview: Physical AI &amp; Humanoid Robotics" class="linkLabel_WmDU">Course Overview: Physical AI &amp; Humanoid Robotics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/why-physical-ai-matters"><span title="Why Physical AI Matters: Bridging the Digital and the Embodied" class="linkLabel_WmDU">Why Physical AI Matters: Bridging the Digital and the Embodied</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/learning-outcomes"><span title="Course Learning Outcomes: Physical AI &amp; Humanoid Robotics" class="linkLabel_WmDU">Course Learning Outcomes: Physical AI &amp; Humanoid Robotics</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/docs/category/module-1-the-robotic-nervous-system-ros-2"><span title="Module 1: The Robotic Nervous System (ROS 2)" class="categoryLinkLabel_W154">Module 1: The Robotic Nervous System (ROS 2)</span></a><button aria-label="Expand sidebar category &#x27;Module 1: The Robotic Nervous System (ROS 2)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/docs/category/module-2-the-digital-twin-gazebo--unity"><span title="Module 2: The Digital Twin (Gazebo &amp; Unity)" class="categoryLinkLabel_W154">Module 2: The Digital Twin (Gazebo &amp; Unity)</span></a><button aria-label="Expand sidebar category &#x27;Module 2: The Digital Twin (Gazebo &amp; Unity)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/docs/category/module-3-the-ai-robot-brain-nvidia-isaac"><span title="Module 3: The AI-Robot Brain (NVIDIA Isaac)" class="categoryLinkLabel_W154">Module 3: The AI-Robot Brain (NVIDIA Isaac)</span></a><button aria-label="Expand sidebar category &#x27;Module 3: The AI-Robot Brain (NVIDIA Isaac)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/docs/category/module-4-vision-language-action-vla"><span title="Module 4: Vision-Language-Action (VLA)" class="categoryLinkLabel_W154">Module 4: Vision-Language-Action (VLA)</span></a><button aria-label="Collapse sidebar category &#x27;Module 4: Vision-Language-Action (VLA)&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/module-4/voice-to-action-whisper"><span title="Vision-Language-Action: Voice-to-Action with Speech Recognition" class="linkLabel_WmDU">Vision-Language-Action: Voice-to-Action with Speech Recognition</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/module-4/cognitive-planning-llm-to-ros2"><span title="Vision-Language-Action: Cognitive Planning with LLMs" class="linkLabel_WmDU">Vision-Language-Action: Cognitive Planning with LLMs</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/module-4/capstone-autonomous-humanoid"><span title="Capstone Project" class="linkLabel_WmDU">Capstone Project</span></a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/weekly-breakdown"><span title="Weekly Breakdown: Physical AI &amp; Humanoid Robotics Course" class="linkLabel_WmDU">Weekly Breakdown: Physical AI &amp; Humanoid Robotics Course</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/assessments"><span title="Course Assessments: Physical AI &amp; Humanoid Robotics" class="linkLabel_WmDU">Course Assessments: Physical AI &amp; Humanoid Robotics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/hardware-requirements"><span title="Hardware Requirements: Physical AI &amp; Humanoid Robotics" class="linkLabel_WmDU">Hardware Requirements: Physical AI &amp; Humanoid Robotics</span></a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/docs/category/module-4-vision-language-action-vla"><span>Module 4: Vision-Language-Action (VLA)</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Vision-Language-Action: Voice-to-Action with Speech Recognition</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="container margin-vert--lg"><div class="doc-content"><header><h1>Vision-Language-Action: Voice-to-Action with Speech Recognition</h1></header>
<p>We&#x27;ve reached the final and most exciting frontier of our curriculum: Vision-Language-Action (VLA) models. This is where we fuse the power of large-scale AI models with the physical embodiment of our robot. The goal is to create a robot that can understand natural human instructions, perceive its environment, and take meaningful action‚Äîa true cognitive robot.</p>
<p>This chapter focuses on the first step of this pipeline: <strong>voice</strong>. How do we enable a robot to listen to a spoken command and understand what was said? We will explore using a <strong>speech-to-text service</strong>, a state-of-the-art automatic speech recognition (ASR) model, to transcribe spoken audio into text. We will then build a ROS 2 node that can listen to a microphone, use this service to transcribe the audio, and publish the resulting text, making the human voice just another sensor in the ROS ecosystem.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-vla-pipeline">The VLA Pipeline<a href="#the-vla-pipeline" class="hash-link" aria-label="Direct link to The VLA Pipeline" title="Direct link to The VLA Pipeline" translate="no">‚Äã</a></h2>
<p>Before diving in, let&#x27;s look at the high-level VLA pipeline we&#x27;ll be building across this module. It&#x27;s a chain of specialized AI models and robotic components working in concert.</p>
<div class="language-mermaid codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-mermaid codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">graph TD</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    A[Human Voice: &quot;Clean the room&quot;] --&gt; B{Microphone};</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    B -- Audio Stream --&gt; C[Speech-to-Text ASR Node];</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    C -- Transcribed Text --&gt; D[LLM Cognitive Planner Node];</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    D -- Sequence of ROS Actions --&gt; E[Robot&#x27;s Control System];</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    subgraph This Chapter</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        B; C;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    end</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    subgraph Next Chapter</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        D;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    end</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    E -- Executes Actions --&gt; F[Physical Robot];</span><br></span></code></pre></div></div>
<p>Our focus here is on the initial stage: converting the continuous audio stream from a microphone into a discrete, textual command.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="why-speech-to-text-for-robotics">Why Speech-to-Text for Robotics?<a href="#why-speech-to-text-for-robotics" class="hash-link" aria-label="Direct link to Why Speech-to-Text for Robotics?" title="Direct link to Why Speech-to-Text for Robotics?" translate="no">‚Äã</a></h2>
<p>A robust Automatic Speech Recognition (ASR) system is crucial for enabling robots to understand natural language commands. Key features of an effective ASR system for robotics include:</p>
<ul>
<li class=""><strong>High Accuracy</strong>: The ability to accurately transcribe speech across various accents, languages, and environmental conditions is paramount for reliable robot operation.</li>
<li class=""><strong>Multilingual Support</strong>: For global applications, the ASR system should ideally support transcription in multiple languages.</li>
<li class=""><strong>Deployment Flexibility</strong>: The option to run the ASR model locally on edge devices (like the NVIDIA Jetson Orin) is often preferred to reduce latency, ensure privacy, and minimize reliance on cloud services. This contrasts with cloud-based solutions, which, while powerful, introduce network dependencies.</li>
<li class=""><strong>Ease of Integration</strong>: A straightforward API or library facilitates seamless integration into existing robotic frameworks, such as ROS 2 nodes.</li>
</ul>
<p>For educational and research purposes, prioritizing open-source or free-tier solutions that can be run locally is often beneficial, as it provides greater control and avoids recurring costs associated with proprietary cloud services.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="lab-1-a-ros-2-node-for-real-time-transcription">Lab 1: A ROS 2 Node for Real-Time Transcription<a href="#lab-1-a-ros-2-node-for-real-time-transcription" class="hash-link" aria-label="Direct link to Lab 1: A ROS 2 Node for Real-Time Transcription" title="Direct link to Lab 1: A ROS 2 Node for Real-Time Transcription" translate="no">‚Äã</a></h2>
<p>In this lab, we&#x27;ll build a &quot;Whisperer&quot; node. This node will listen to a microphone, accumulate audio until it detects silence, and then run the Whisper model to transcribe the captured audio segment.</p>
<p><strong>Prerequisites</strong>:</p>
<ul>
<li class="">A working microphone connected to your machine.</li>
<li class="">A Python environment with a suitable speech-to-text library installed (e.g., <code>SpeechRecognition</code> combined with a local STT engine, or an API client for a cloud STT service).</li>
<li class="">Audio libraries like <code>PortAudio</code> (often needed for <code>SpeechRecognition</code>): <code>sudo apt-get install portaudio19-dev python3-pyaudio</code>.</li>
<li class="">A ROS 2 workspace.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-1-implement-your-speech-to-text-ros-2-node">Step 1: Implement Your Speech-to-Text ROS 2 Node<a href="#step-1-implement-your-speech-to-text-ros-2-node" class="hash-link" aria-label="Direct link to Step 1: Implement Your Speech-to-Text ROS 2 Node" title="Direct link to Step 1: Implement Your Speech-to-Text ROS 2 Node" translate="no">‚Äã</a></h3>
<p>Implement a ROS 2 node that utilizes your chosen speech-to-text library or API. This node should:</p>
<ol>
<li class="">Listen for audio input from a microphone.</li>
<li class="">Process the audio to convert speech into text.</li>
<li class="">Publish the transcribed text to a ROS 2 topic (e.g., <code>/transcribed_text</code>).</li>
</ol>
<p><strong>(Note: The previous code example using OpenAI Whisper has been removed. You will need to implement this section using a non-OpenAI speech-to-text solution. Consider using <code>speech_recognition</code> library with a local engine or a client for a free-tier cloud STT service.)</strong></p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-2-add-entry-point-and-build">Step 2: Add Entry Point and Build<a href="#step-2-add-entry-point-and-build" class="hash-link" aria-label="Direct link to Step 2: Add Entry Point and Build" title="Direct link to Step 2: Add Entry Point and Build" translate="no">‚Äã</a></h3>
<p>Add the <code>whisper_node</code> to your <code>setup.py</code> and build your workspace with <code>colcon build</code>.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-3-run-and-test-your-speech-to-text-node">Step 3: Run and Test Your Speech-to-Text Node<a href="#step-3-run-and-test-your-speech-to-text-node" class="hash-link" aria-label="Direct link to Step 3: Run and Test Your Speech-to-Text Node" title="Direct link to Step 3: Run and Test Your Speech-to-Text Node" translate="no">‚Äã</a></h3>
<ol>
<li class="">Source your workspace: <code>source install/setup.bash</code>.</li>
<li class="">Run your speech-to-text node (e.g., <code>ros2 run voice_agent_py your_stt_node_name</code>).</li>
<li class="">In another terminal, listen to the output topic: <code>ros2 topic echo /transcribed_text</code>.</li>
<li class="">Speak a command into your microphone.</li>
<li class="">You should see the transcribed text appear in the <code>ros2 topic echo</code> terminal.</li>
</ol>
<div align="center"><p><em>Image: Terminal outputs of the Whisper node and ROS 2 topic echo.</em>
<em>A screenshot showing two terminals. The left terminal shows the output of the <code>whisper_node</code>, with logs like &quot;Whisper transcribed: &#x27;Hello robot, please clean the room.&#x27;&quot;. The right terminal shows the output of <code>ros2 topic echo</code>, displaying the same text as a ROS message.</em></p></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="common-pitfalls-and-best-practices-for-speech-to-text">Common Pitfalls and Best Practices for Speech-to-Text<a href="#common-pitfalls-and-best-practices-for-speech-to-text" class="hash-link" aria-label="Direct link to Common Pitfalls and Best Practices for Speech-to-Text" title="Direct link to Common Pitfalls and Best Practices for Speech-to-Text" translate="no">‚Äã</a></h2>
<ol>
<li class="">
<p><strong>Pitfall</strong>: The transcription is inaccurate or picks up a lot of background noise.</p>
<ul>
<li class=""><strong>Cause</strong>: Poor microphone quality, a noisy environment, or ineffective ambient noise calibration.</li>
<li class=""><strong>Fix</strong>: Use a better quality microphone (headsets are often better than built-in laptop mics). Run the node in a quieter room. Ensure your speech recognition library&#x27;s ambient noise adjustment is functioning correctly.</li>
</ul>
</li>
<li class="">
<p><strong>Pitfall</strong>: The speech-to-text processing is slow.</p>
<ul>
<li class=""><strong>Cause</strong>: The chosen speech-to-text model is computationally intensive, or hardware resources are limited.</li>
<li class=""><strong>Fix</strong>: If using a local model, consider using a smaller model or leveraging GPU acceleration if available. If using a cloud service, check your internet connection and API response times.</li>
</ul>
</li>
<li class="">
<p><strong>Best Practice: Keyword Spotting</strong>: In many robotic applications, you want the robot to respond only when a specific &quot;wake word&quot; or phrase is spoken. You can implement this by using a lightweight keyword spotting library to trigger the full speech-to-text transcription only after the wake word is detected.</p>
</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="further-reading">Further Reading<a href="#further-reading" class="hash-link" aria-label="Direct link to Further Reading" title="Direct link to Further Reading" translate="no">‚Äã</a></h2>
<ul>
<li class=""><strong><code>speech_recognition</code> Library PyPI</strong>: <a href="https://pypi.org/project/SpeechRecognition/" target="_blank" rel="noopener noreferrer" class="">https://pypi.org/project/SpeechRecognition/</a></li>
<li class=""><strong>ROS 2 <code>std_msgs/String</code></strong>: <a href="http://docs.ros.org/en/humble/API/std_msgs/msg/String.html" target="_blank" rel="noopener noreferrer" class="">http://docs.ros.org/en/humble/API/std_msgs/msg/String.html</a></li>
<li class=""><strong>Picovoice for Wake Word/Keyword Spotting</strong>: A popular library for creating custom wake words. <a href="https://picovoice.ai/" target="_blank" rel="noopener noreferrer" class="">https://picovoice.ai/</a></li>
</ul>
<hr>
<p><a class="" href="/docs/module-3/nav2-bipedal-path-planning"><strong>‚Üê Previous: Nav2 for Bipedal Path Planning</strong></a> | <a class="" href="/docs/module-4/cognitive-planning-llm-to-ros2"><strong>Next: Cognitive Planning with LLMs ‚Üí</strong></a></p></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/SyedAbdulSami1/Physical-AI-Humanoid-Robotics-Textbook/tree/main/docs/module-4/voice-to-action-whisper.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/category/module-4-vision-language-action-vla"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Module 4: Vision-Language-Action (VLA)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/module-4/cognitive-planning-llm-to-ros2"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Vision-Language-Action: Cognitive Planning with LLMs</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#the-vla-pipeline" class="table-of-contents__link toc-highlight">The VLA Pipeline</a></li><li><a href="#why-speech-to-text-for-robotics" class="table-of-contents__link toc-highlight">Why Speech-to-Text for Robotics?</a></li><li><a href="#lab-1-a-ros-2-node-for-real-time-transcription" class="table-of-contents__link toc-highlight">Lab 1: A ROS 2 Node for Real-Time Transcription</a><ul><li><a href="#step-1-implement-your-speech-to-text-ros-2-node" class="table-of-contents__link toc-highlight">Step 1: Implement Your Speech-to-Text ROS 2 Node</a></li><li><a href="#step-2-add-entry-point-and-build" class="table-of-contents__link toc-highlight">Step 2: Add Entry Point and Build</a></li><li><a href="#step-3-run-and-test-your-speech-to-text-node" class="table-of-contents__link toc-highlight">Step 3: Run and Test Your Speech-to-Text Node</a></li></ul></li><li><a href="#common-pitfalls-and-best-practices-for-speech-to-text" class="table-of-contents__link toc-highlight">Common Pitfalls and Best Practices for Speech-to-Text</a></li><li><a href="#further-reading" class="table-of-contents__link toc-highlight">Further Reading</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Content</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Textbook</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/gemini-cli/Physical-AI-Humanoid-Robotics-Textbook" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright ¬© 2025 gemini-cli. Built with Docusaurus.</div></div></div></footer><div class="chatbot-container"><button class="chatbot-toggle-button">ü§ñ</button></div></div>
</body>
</html>