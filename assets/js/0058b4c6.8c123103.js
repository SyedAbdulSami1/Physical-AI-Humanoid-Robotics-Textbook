"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[849],{6164:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","href":"/docs/intro","label":"Course Overview: Physical AI & Humanoid Robotics","docId":"intro","unlisted":false},{"type":"link","href":"/docs/why-physical-ai-matters","label":"Why Physical AI Matters: Bridging the Digital and the Embodied","docId":"why-physical-ai-matters","unlisted":false},{"type":"link","href":"/docs/learning-outcomes","label":"Course Learning Outcomes: Physical AI & Humanoid Robotics","docId":"learning-outcomes","unlisted":false},{"type":"category","label":"Module 1: The Robotic Nervous System (ROS 2)","items":[{"type":"link","href":"/docs/module-1/ros2-nodes-topics-services","label":"ROS 2 Fundamentals: Nodes, Topics, and Services","docId":"module-1/ros2-nodes-topics-services","unlisted":false},{"type":"link","href":"/docs/module-1/bridging-python-agents-to-ros2","label":"Bridging Python Agents to ROS 2 Controllers with rclpy","docId":"module-1/bridging-python-agents-to-ros2","unlisted":false},{"type":"link","href":"/docs/module-1/urdf-for-humanoids","label":"Understanding URDF for Humanoid Robots: Anatomy of a Digital Body","docId":"module-1/urdf-for-humanoids","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/docs/category/module-1-the-robotic-nervous-system-ros-2"},{"type":"category","label":"Module 2: The Digital Twin (Gazebo & Unity)","items":[{"type":"link","href":"/docs/module-2/simulating-physics-in-gazebo","label":"The Digital Twin: Simulating Physics in Gazebo","docId":"module-2/simulating-physics-in-gazebo","unlisted":false},{"type":"link","href":"/docs/module-2/high-fidelity-rendering-in-unity","label":"High-Fidelity Rendering and Human-Robot Interaction in Unity","docId":"module-2/high-fidelity-rendering-in-unity","unlisted":false},{"type":"link","href":"/docs/module-2/simulating-sensors-lidar-depth-imu","label":"The Digital Twin: Simulating Sensors (LiDAR, Depth, IMU)","docId":"module-2/simulating-sensors-lidar-depth-imu","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/docs/category/module-2-the-digital-twin-gazebo--unity"},{"type":"category","label":"Module 3: The AI-Robot Brain (NVIDIA Isaac)","items":[{"type":"link","href":"/docs/module-3/isaac-sim-synthetic-data","label":"The AI-Robot Brain: Isaac Sim for Synthetic Data","docId":"module-3/isaac-sim-synthetic-data","unlisted":false},{"type":"link","href":"/docs/module-3/isaac-ros-vslam-navigation","label":"The AI-Robot Brain: Isaac ROS for VSLAM & Navigation","docId":"module-3/isaac-ros-vslam-navigation","unlisted":false},{"type":"link","href":"/docs/module-3/nav2-bipedal-path-planning","label":"The AI-Robot Brain: Nav2 for Bipedal Path Planning","docId":"module-3/nav2-bipedal-path-planning","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/docs/category/module-3-the-ai-robot-brain-nvidia-isaac"},{"type":"category","label":"Module 4: Vision-Language-Action (VLA)","items":[{"type":"link","href":"/docs/module-4/voice-to-action-whisper","label":"Vision-Language-Action: Voice-to-Action with Speech Recognition","docId":"module-4/voice-to-action-whisper","unlisted":false},{"type":"link","href":"/docs/module-4/cognitive-planning-llm-to-ros2","label":"Vision-Language-Action: Cognitive Planning with LLMs","docId":"module-4/cognitive-planning-llm-to-ros2","unlisted":false},{"type":"link","href":"/docs/module-4/capstone-autonomous-humanoid","label":"Capstone Project","docId":"module-4/capstone-autonomous-humanoid","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/docs/category/module-4-vision-language-action-vla"},{"type":"link","href":"/docs/weekly-breakdown","label":"Weekly Breakdown: Physical AI & Humanoid Robotics Course","docId":"weekly-breakdown","unlisted":false},{"type":"link","href":"/docs/assessments","label":"Course Assessments: Physical AI & Humanoid Robotics","docId":"assessments","unlisted":false},{"type":"link","href":"/docs/hardware-requirements","label":"Hardware Requirements: Physical AI & Humanoid Robotics","docId":"hardware-requirements","unlisted":false}]},"docs":{"assessments":{"id":"assessments","title":"Course Assessments: Physical AI & Humanoid Robotics","description":"The assessment strategy for \\"Physical AI & Humanoid Robotics: AI Systems in the Physical World and Embodied Intelligence\\" is designed to rigorously evaluate students\' comprehension of theoretical concepts and their ability to apply them in practical, hands-on robotic development. As a capstone course, the assessments emphasize project-based learning, problem-solving, and the integration of complex systems, reflecting the multidisciplinary nature of Physical AI. Success in this course hinges on demonstrating not only individual technical proficiency but also the capacity to build functional, intelligent robotic behaviors.","sidebar":"tutorialSidebar"},"hardware-requirements":{"id":"hardware-requirements","title":"Hardware Requirements: Physical AI & Humanoid Robotics","description":"The \\"Physical AI & Humanoid Robotics\\" course is inherently technically demanding, as it combines high-fidelity physics simulation, sophisticated visual perception, and advanced generative AI models. To ensure a productive and seamless learning experience, specific hardware configurations are essential. These requirements are designed to provide students with the necessary computational power to run complex simulations, develop real-time AI algorithms, and interact with robotic platforms effectively. Attempting the course with significantly under-specced hardware may lead to frustrating performance bottlenecks and hinder the learning process.","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Course Overview: Physical AI & Humanoid Robotics","description":"Welcome to \\"Physical AI & Humanoid Robotics: AI Systems in the Physical World and Embodied Intelligence,\\" a capstone quarter course designed to bridge the chasm between artificial intelligence and the physical domain. In an era where digital AI has achieved remarkable feats in virtual environments, the next great frontier lies in empowering AI systems to operate, understand, and interact seamlessly within the complexities of our tangible world. This course provides a comprehensive exploration of Physical AI, a transformative field that fuses advanced AI methodologies with the engineering challenges of robotics to create intelligent machines capable of embodied interaction.","sidebar":"tutorialSidebar"},"learning-outcomes":{"id":"learning-outcomes","title":"Course Learning Outcomes: Physical AI & Humanoid Robotics","description":"This course, \\"Physical AI & Humanoid Robotics: AI Systems in the Physical World and Embodied Intelligence,\\" is meticulously designed to equip students with a comprehensive understanding and practical skill set crucial for navigating the rapidly evolving landscape of embodied AI. Upon successful completion of this capstone quarter, students will be able to demonstrate proficiency across several key domains, bridging theoretical knowledge with hands-on application in robotic systems. The learning outcomes are structured to ensure a holistic grasp of the principles, technologies, and methodologies essential for designing, simulating, and deploying intelligent robots capable of operating in complex physical environments.","sidebar":"tutorialSidebar"},"module-1/bridging-python-agents-to-ros2":{"id":"module-1/bridging-python-agents-to-ros2","title":"Bridging Python Agents to ROS 2 Controllers with rclpy","description":"The Nexus of AI and Robotics","sidebar":"tutorialSidebar"},"module-1/ros2-nodes-topics-services":{"id":"module-1/ros2-nodes-topics-services","title":"ROS 2 Fundamentals: Nodes, Topics, and Services","description":"Introduction to the Robotic Nervous System","sidebar":"tutorialSidebar"},"module-1/urdf-for-humanoids":{"id":"module-1/urdf-for-humanoids","title":"Understanding URDF for Humanoid Robots: Anatomy of a Digital Body","description":"Introduction to the Unified Robot Description Format","sidebar":"tutorialSidebar"},"module-2/high-fidelity-rendering-in-unity":{"id":"module-2/high-fidelity-rendering-in-unity","title":"High-Fidelity Rendering and Human-Robot Interaction in Unity","description":"While Gazebo provides a robust physics simulation, Unity excels at creating photorealistic, visually immersive, and highly interactive environments. For Physical AI, especially in the domain of humanoid robotics, high-fidelity rendering is not just an aesthetic enhancement\u2014it\'s a critical component for training and validating perception systems, generating realistic synthetic data, and designing intuitive Human-Robot Interaction (HRI) scenarios.","sidebar":"tutorialSidebar"},"module-2/simulating-physics-in-gazebo":{"id":"module-2/simulating-physics-in-gazebo","title":"The Digital Twin: Simulating Physics in Gazebo","description":"Welcome to the world of the digital twin, where we forge a virtual copy of our robot to test, train, and iterate without risking physical hardware. Our primary tool for this is Gazebo, a powerful 3D robotics simulator that allows us to model not just the robot, but the world it lives in\u2014complete with gravity, friction, and collisions.","sidebar":"tutorialSidebar"},"module-2/simulating-sensors-lidar-depth-imu":{"id":"module-2/simulating-sensors-lidar-depth-imu","title":"The Digital Twin: Simulating Sensors (LiDAR, Depth, IMU)","description":"A robot is blind and numb without its sensors. To develop an intelligent agent, we need to provide it with a stream of data that accurately represents what it would perceive in the real world. In simulation, this is achieved through sensor plugins. These are small, loadable modules that attach to a robot\'s model and generate realistic sensor data based on the state of the simulated world.","sidebar":"tutorialSidebar"},"module-3/isaac-ros-vslam-navigation":{"id":"module-3/isaac-ros-vslam-navigation","title":"The AI-Robot Brain: Isaac ROS for VSLAM & Navigation","description":"Once we have a simulated robot that can perceive its world, the next step is to give it autonomy. How does a robot know where it is? And how does it get from point A to point B? This chapter dives into the NVIDIA Isaac ROS packages, a collection of hardware-accelerated ROS 2 packages specifically designed for AI-based robotics.","sidebar":"tutorialSidebar"},"module-3/isaac-sim-synthetic-data":{"id":"module-3/isaac-sim-synthetic-data","title":"The AI-Robot Brain: Isaac Sim for Synthetic Data","description":"Welcome to the core of our AI-powered robotics curriculum generating high-quality, physically-accurate synthetic data at scale. It combines the power of NVIDIA\'s Omniverse platform for photorealistic rendering with advanced physics simulation (PhysX 5) and tight integration with the ROS ecosystem.","sidebar":"tutorialSidebar"},"module-3/nav2-bipedal-path-planning":{"id":"module-3/nav2-bipedal-path-planning","title":"The AI-Robot Brain: Nav2 for Bipedal Path Planning","description":"Navigating a wheeled robot like the Carter is a well-solved problem. Navigating a bipedal humanoid, however, is a monumental challenge. Humanoids are inherently unstable, have complex kinematics, and interact with the world in a much more dynamic way. While the standard Nav2 stack is a great starting point, applying it to humanoids requires special considerations and custom plugins.","sidebar":"tutorialSidebar"},"module-4/capstone-autonomous-humanoid":{"id":"module-4/capstone-autonomous-humanoid","title":"Capstone: The Autonomous Humanoid","description":"Welcome to the final and most exciting part of our journey into Physical AI. This capstone project, \\"The Autonomous Humanoid,\\" is where you will integrate everything you\'ve learned across all four modules. You will build and program a simulated humanoid robot that can understand a natural language voice command, perceive its environment, plan a complex series of actions, navigate through space, and physically interact with an object.","sidebar":"tutorialSidebar"},"module-4/cognitive-planning-llm-to-ros2":{"id":"module-4/cognitive-planning-llm-to-ros2","title":"Vision-Language-Action: Cognitive Planning with LLMs","description":"We have successfully converted a spoken command into text. Now comes the \\"cognitive\\" part of our VLA pipeline. How does a robot understand the intent behind a command like \\"Clean the room\\" and translate that high-level goal into a concrete sequence of physical actions? This is where Large Language Models (LLMs) like Google\'s Gemini come into play.","sidebar":"tutorialSidebar"},"module-4/voice-to-action-whisper":{"id":"module-4/voice-to-action-whisper","title":"Vision-Language-Action: Voice-to-Action with Speech Recognition","description":"We\'ve reached the final and most exciting frontier of our curriculum: Vision-Language-Action (VLA) models. This is where we fuse the power of large-scale AI models with the physical embodiment of our robot. The goal is to create a robot that can understand natural human instructions, perceive its environment, and take meaningful action\u2014a true cognitive robot.","sidebar":"tutorialSidebar"},"pitfalls_and_fixes":{"id":"pitfalls_and_fixes","title":"Common Pitfalls and Fixes for Physical AI & Humanoid Robotics Textbook RAG Chatbot","description":"Backend Issues"},"weekly-breakdown":{"id":"weekly-breakdown","title":"Weekly Breakdown: Physical AI & Humanoid Robotics Course","description":"This 13-week capstone course, \\"Physical AI & Humanoid Robotics: AI Systems in the Physical World and Embodied Intelligence,\\" is structured to provide a comprehensive and intensive learning experience. Each week builds upon the preceding material, guiding students from foundational concepts to advanced applications, culminating in the design and implementation of sophisticated embodied AI systems. The curriculum balances theoretical understanding with practical, hands-on development, preparing students for real-world challenges in robotics.","sidebar":"tutorialSidebar"},"why-physical-ai-matters":{"id":"why-physical-ai-matters","title":"Why Physical AI Matters: Bridging the Digital and the Embodied","description":"The advent of artificial intelligence has profoundly reshaped our digital world, empowering systems to process vast datasets, recognize patterns, and automate complex cognitive tasks. However, the true frontier of AI lies not just in the digital realm but in its seamless integration with the physical world. This is the domain of Physical AI \u2013 a transformative discipline focused on creating AI systems that can operate intelligently and autonomously within our tangible reality, understanding and interacting with physical laws.","sidebar":"tutorialSidebar"}}}}')}}]);