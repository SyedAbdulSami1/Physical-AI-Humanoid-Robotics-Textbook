"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[2024],{1894:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-4/capstone-autonomous-humanoid","title":"Capstone: The Autonomous Humanoid","description":"Welcome to the final and most exciting part of our journey into Physical AI. This capstone project, \\"The Autonomous Humanoid,\\" is where you will integrate everything you\'ve learned across all four modules. You will build and program a simulated humanoid robot that can understand a natural language voice command, perceive its environment, plan a complex series of actions, navigate through space, and physically interact with an object.","source":"@site/docs/module-4/capstone-autonomous-humanoid.md","sourceDirName":"module-4","slug":"/module-4/capstone-autonomous-humanoid","permalink":"/docs/module-4/capstone-autonomous-humanoid","draft":false,"unlisted":false,"editUrl":"https://github.com/SyedAbdulSami1/Physical-AI-Humanoid-Robotics-Textbook/tree/main/docs/module-4/capstone-autonomous-humanoid.md","tags":[],"version":"current","frontMatter":{"title":"Capstone: The Autonomous Humanoid","sidebar_label":"Capstone Project"},"sidebar":"tutorialSidebar","previous":{"title":"Vision-Language-Action: Cognitive Planning with LLMs","permalink":"/docs/module-4/cognitive-planning-llm-to-ros2"},"next":{"title":"Weekly Breakdown: Physical AI & Humanoid Robotics Course","permalink":"/docs/weekly-breakdown"}}');var t=o(4848),a=o(8453);const s={title:"Capstone: The Autonomous Humanoid",sidebar_label:"Capstone Project"},r="Module 4 Capstone: The Autonomous Humanoid",l={},c=[{value:"Project Overview",id:"project-overview",level:2},{value:"Core Components &amp; Architecture",id:"core-components--architecture",level:2},{value:"Recommended Repository Structure",id:"recommended-repository-structure",level:2},{value:"Step-by-Step Implementation Guide",id:"step-by-step-implementation-guide",level:2},{value:"1. Environment Setup",id:"1-environment-setup",level:3},{value:"2. Robot URDF &amp; Simulation",id:"2-robot-urdf--simulation",level:3},{value:"3. Voice-to-Action Node (<code>voice_to_action_node.py</code>)",id:"3-voice-to-action-node-voice_to_action_nodepy",level:3},{value:"4. LLM Planner Node (<code>llm_planner_node.py</code>)",id:"4-llm-planner-node-llm_planner_nodepy",level:3},{value:"5. Perception Node (<code>perception_node.py</code>)",id:"5-perception-node-perception_nodepy",level:3},{value:"6. Main Launch File (<code>capstone_project.launch.py</code>)",id:"6-main-launch-file-capstone_projectlaunchpy",level:3},{value:"Video Demo Instructions",id:"video-demo-instructions",level:2},{value:"Grading Rubric",id:"grading-rubric",level:2},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-capstone-the-autonomous-humanoid",children:"Module 4 Capstone: The Autonomous Humanoid"})}),"\n",(0,t.jsx)(n.p,{children:'Welcome to the final and most exciting part of our journey into Physical AI. This capstone project, "The Autonomous Humanoid," is where you will integrate everything you\'ve learned across all four modules. You will build and program a simulated humanoid robot that can understand a natural language voice command, perceive its environment, plan a complex series of actions, navigate through space, and physically interact with an object.'}),"\n",(0,t.jsx)(n.p,{children:"This project represents the culmination of this course, bridging the digital brain (AI) with the physical body (robot) to achieve true embodied intelligence."}),"\n",(0,t.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,t.jsx)(n.p,{children:"The goal of this project is to create an autonomous system where a humanoid robot in a simulated environment can execute a high-level task given by a human user via voice."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"The Scenario:"})}),"\n",(0,t.jsxs)(n.p,{children:["The robot is in a simulated room containing a few objects (e.g., a table, a chair, and a can of soda on the table). The user gives a voice command like: ",(0,t.jsx)(n.strong,{children:'"Hey robot, please pick up the soda can."'})]}),"\n",(0,t.jsx)(n.p,{children:"The robot must then:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Listen and Understand"}),": Transcribe the voice command to text."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Think and Plan"}),": Use a Large Language Model (LLM) to break down the command into a sequence of executable robotic actions."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"See and Locate"}),": Visually search the room to find the soda can."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Walk and Navigate"}),": Plan a path to the table where the soda can is located and walk there, avoiding obstacles."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Reach and Grasp"}),": Use its arm to pick up the soda can."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This end-to-end task demonstrates a complete Vision-Language-Action (VLA) pipeline."}),"\n",(0,t.jsx)(n.h2,{id:"core-components--architecture",children:"Core Components & Architecture"}),"\n",(0,t.jsx)(n.p,{children:"Your system will be a distributed network of ROS 2 nodes, each responsible for a specific part of the task."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:'graph TD\n    A[User Voice Command] --\x3e B(Whisper Node: Speech-to-Text);\n    B --\x3e C{LLM Planner Node};\n    C -- Goal: "Find the soda can" --\x3e D[Perception Node (YOLO)];\n    D -- Object Location --\x3e C;\n    C -- Goal: "Navigate to [location]" --\x3e E[Navigation Stack (Nav2)];\n    E -- Navigation Success --\x3e C;\n    C -- Goal: "Grasp at [location]" --\x3e F[Manipulation Stack (MoveIt2)];\n    F -- Grasp Success --\x3e C;\n    C -- Task Complete --\x3e G(Status Node: Announce Completion);\n'})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Voice-to-Action (Speech-to-Text Node)"}),": A ROS 2 node that uses a speech-to-text library to listen for a voice command from a microphone, transcribe it to text, and publish it to a topic."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cognitive Planning (LLM Planner Node)"}),': The "brain" of the robot. This node subscribes to the transcribed text. It then queries a powerful LLM (like Google\'s Gemini) with a carefully crafted prompt to generate a step-by-step plan of ROS 2 actions. It acts as a state machine, executing each step of the plan in sequence.']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception (Perception Node)"}),": This node processes images from the robot's head-mounted camera. It uses an object detection model (e.g., YOLOv8) to identify and locate objects in the environment, publishing their 3D coordinates."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Navigation (Nav2)"}),": You will configure and launch the standard ROS 2 Navigation stack (Nav2) to handle bipedal path planning and locomotion, enabling the robot to walk to a specified coordinate."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Manipulation (MoveIt2)"}),": You will configure and launch the ROS 2 Manipulation stack (MoveIt2) to control the robot's arm, enabling it to plan and execute a grasp on the target object."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"recommended-repository-structure",children:"Recommended Repository Structure"}),"\n",(0,t.jsx)(n.p,{children:"A clean and organized repository is crucial. Use the following structure for your ROS 2 workspace:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"autonomous_humanoid_ws/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 humanoid_bringup/\n\u2502   \u2502   \u251c\u2500\u2500 launch/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 capstone_project.launch.py\n\u2502   \u2502   \u251c\u2500\u2500 worlds/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 aihome.world\n\u2502   \u2502   \u2514\u2500\u2500 rviz/\n\u2502   \u2502       \u2514\u2500\u2500 humanoid_config.rviz\n\u2502   \u251c\u2500\u2500 humanoid_control/\n\u2502   \u2502   \u2514\u2500\u2500 ... (Controller configurations)\n\u2502   \u251c\u2500\u2500 humanoid_description/\n\u2502   \u2502   \u2514\u2500\u2500 urdf/\n\u2502   \u2502       \u2514\u2500\u2500 humanoid_robot.urdf\n\u2502   \u251c\u2500\u2500 humanoid_navigation/\n\u2502   \u2502   \u251c\u2500\u2500 launch/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 nav2.launch.py\n\u2502   \u2502   \u2514\u2500\u2500 params/\n\u2502   \u2502       \u2514\u2500\u2500 nav2_params.yaml\n\u2502   \u251c\u2500\u2500 humanoid_manipulation/\n\u2502   \u2502   \u251c\u2500\u2500 launch/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 moveit.launch.py\n\u2502   \u2502   \u2514\u2500\u2500 config/\n\u2502   \u2502       \u2514\u2500\u2500 ... (MoveIt2 config files)\n\u2502   \u2514\u2500\u2500 humanoid_ai/\n\u2502       \u251c\u2500\u2500 package.xml\n\u2502       \u251c\u2500\u2500 setup.py\n\u2502       \u2514\u2500\u2500 humanoid_ai/\n\u2502           \u251c\u2500\u2500 llm_planner_node.py\n\u2502           \u251c\u2500\u2500 perception_node.py\n\u2502           \u2514\u2500\u2500 voice_to_action_node.py\n\u2514\u2500\u2500 README.md\n"})}),"\n",(0,t.jsx)(n.h2,{id:"step-by-step-implementation-guide",children:"Step-by-Step Implementation Guide"}),"\n",(0,t.jsx)(n.h3,{id:"1-environment-setup",children:"1. Environment Setup"}),"\n",(0,t.jsx)(n.p,{children:"Ensure you have a working ROS 2 Humble, Gazebo, and all necessary Python libraries installed."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Install key Python libraries\npip install ultralytics google-genai\n"})}),"\n",(0,t.jsx)(n.h3,{id:"2-robot-urdf--simulation",children:"2. Robot URDF & Simulation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"URDF"}),": Finalize the URDF for your humanoid robot from Module 1. Ensure it has proper joint limits, inertia, and collision models. Add a camera sensor plugin."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Gazebo World"}),": Create a simple Gazebo world (",(0,t.jsx)(n.code,{children:"aihome.world"}),") with a ground plane, a table, a chair, and a model for the soda can."]}),"\n"]}),"\n",(0,t.jsxs)(n.h3,{id:"3-voice-to-action-node-voice_to_action_nodepy",children:["3. Voice-to-Action Node (",(0,t.jsx)(n.code,{children:"voice_to_action_node.py"}),")"]}),"\n",(0,t.jsx)(n.p,{children:"This node should:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Initialize ",(0,t.jsx)(n.code,{children:"rclpy"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Create a publisher for the transcribed text (",(0,t.jsx)(n.code,{children:"/voice_command"}),")."]}),"\n",(0,t.jsx)(n.li,{children:"Use a suitable speech-to-text library to capture audio and get the transcription."}),"\n",(0,t.jsx)(n.li,{children:"Publish the result."}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# humanoid_ai/voice_to_action_node.py (Snippet)\n# Implement your chosen speech-to-text solution here.\n# Example using a generic STT library:\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport speech_recognition as sr # Example library\nimport sounddevice as sd\nimport numpy as np\nimport scipy.io.wavfile as wav\nimport tempfile\nimport os\n\nclass VoiceToActionNode(Node):\n    def __init__(self):\n        super().__init__('voice_to_action_node')\n        self.publisher_ = self.create_publisher(String, '/voice_command', 10)\n        self.get_logger().info('Voice command node started. Listening...')\n        self.recognizer = sr.Recognizer()\n        self.listen_and_transcribe()\n\n    def listen_and_transcribe(self):\n        with sr.Microphone() as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n            self.get_logger().info(\"Say something!\")\n            audio = self.recognizer.listen(source)\n\n        try:\n            # Example: using Google Web Speech API (requires internet, may have usage limits)\n            text = self.recognizer.recognize_google(audio)\n            self.get_logger().info(f'Heard: {text}')\n            msg = String()\n            msg.data = text\n            self.publisher_.publish(msg)\n        except sr.UnknownValueError:\n            self.get_logger().warn(\"Speech Recognition could not understand audio\")\n        except sr.RequestError as e:\n            self.get_logger().error(f\"Could not request results from Speech Recognition service; {e}\")\n\n# main function and rclpy init/shutdown omitted for brevity\n"})}),"\n",(0,t.jsxs)(n.h3,{id:"4-llm-planner-node-llm_planner_nodepy",children:["4. LLM Planner Node (",(0,t.jsx)(n.code,{children:"llm_planner_node.py"}),")"]}),"\n",(0,t.jsx)(n.p,{children:"This is the most complex node. It acts as the brain and coordinates the other components."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# humanoid_ai/llm_planner_node.py (Snippet)\nimport google.genai as genai\nimport os\n\nclass LLMPlannerNode(Node):\n    def __init__(self):\n        super().__init__(\'llm_planner_node\')\n        self.subscription = self.create_subscription(\n            String, \'/voice_command\', self.command_callback, 10)\n        # Add clients for Nav2 and MoveIt2 actions\n        # Add publisher to send goals\n        self.state = "IDLE"\n        \n        # --- LLM Setup ---\n        try:\n            genai.configure(api_key=os.environ["GEMINI_API_KEY"])\n            self.model = genai.GenerativeModel(\'gemini-pro\')\n        except Exception as e:\n            self.get_logger().error(f"Failed to initialize Gemini client: {e}")\n            self.get_logger().error("Please make sure the GEMINI_API_KEY environment variable is set.")\n            return\n\n\n    def command_callback(self, msg):\n        if self.state == "IDLE":\n            self.get_logger().info(f\'Received command: "{msg.data}"\')\n            self.state = "PLANNING"\n            self.generate_plan(msg.data)\n\n    def generate_plan(self, command):\n        prompt = f"""\n        You are the cognitive core for a humanoid robot.\n        Translate the user\'s command into a numbered list of robotic actions.\n        Available actions:\n        - find_object(object_name)\n        - go_to(x, y, z)\n        - pick_up(object_name)\n        - done()\n\n        Command: "{command}"\n        Plan:\n        """\n        # Call Gemini API with this prompt\n        response = self.model.generate_content(prompt)\n\n        # For this example, let\'s hardcode the plan\n        plan = [\n            "1. find_object(\'soda_can\')",\n            "2. go_to(2.5, 1.0, 0.8)", # Assume object location is returned\n            "3. pick_up(\'soda_can\')",\n            "4. done()"\n        ]\n        self.execute_plan(plan)\n\n    def execute_plan(self, plan):\n        # A state machine to execute each step of the plan\n        # This involves calling ROS 2 actions and services\n        self.get_logger().info("Executing plan...")\n        # ... implementation of plan execution ...\n'})}),"\n",(0,t.jsxs)(n.h3,{id:"5-perception-node-perception_nodepy",children:["5. Perception Node (",(0,t.jsx)(n.code,{children:"perception_node.py"}),")"]}),"\n",(0,t.jsx)(n.p,{children:"This node uses a pre-trained YOLO model to find objects."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# humanoid_ai/perception_node.py (Snippet)\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nfrom ultralytics import YOLO\n\nclass PerceptionNode(Node):\n    def __init__(self):\n        super().__init__('perception_node')\n        self.model = YOLO('yolov8n.pt')  # Load pretrained model\n        self.bridge = CvBridge()\n        self.subscription = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10)\n        self.object_publisher = self.create_publisher(...) # Custom message for object location\n\n    def image_callback(self, msg):\n        cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n        results = self.model(cv_image)\n\n        for r in results:\n            boxes = r.boxes\n            for box in boxes:\n                # Get class name\n                cls = int(box.cls[0])\n                class_name = self.model.names[cls]\n                if class_name == 'soda can': # Or whatever YOLO calls it\n                    # ... calculate 3D position and publish it\n                    self.get_logger().info(f'Found a soda can!')\n"})}),"\n",(0,t.jsxs)(n.h3,{id:"6-main-launch-file-capstone_projectlaunchpy",children:["6. Main Launch File (",(0,t.jsx)(n.code,{children:"capstone_project.launch.py"}),")"]}),"\n",(0,t.jsx)(n.p,{children:"This file brings everything together: Gazebo, Nav2, MoveIt2, and your custom AI nodes."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# humanoid_bringup/launch/capstone_project.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    humanoid_ai_pkg = get_package_share_directory('humanoid_ai')\n    nav_pkg = get_package_share_directory('humanoid_navigation')\n\n    return LaunchDescription([\n        # Launch Gazebo, Nav2, MoveIt2...\n        # ...\n        \n        # Launch AI Nodes\n        Node(\n            package='humanoid_ai',\n            executable='voice_to_action_node',\n            name='voice_to_action_node',\n            output='screen'),\n        Node(\n            package='humanoid_ai',\n            executable='llm_planner_node',\n            name='llm_planner_node',\n            output='screen'),\n        Node(\n            package='humanoid_ai',\n            executable='perception_node',\n            name='perception_node',\n            output='screen'),\n    ])\n"})}),"\n",(0,t.jsx)(n.h2,{id:"video-demo-instructions",children:"Video Demo Instructions"}),"\n",(0,t.jsx)(n.p,{children:"You must create a 3-5 minute video demonstrating your final project. The video should include:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Introduction (15s)"}),": Briefly introduce yourself and the project."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Code Walkthrough (60s)"}),": Briefly show your repository structure and highlight a key section of your ",(0,t.jsx)(n.code,{children:"llm_planner_node.py"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Live Demo (2-3 mins)"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Show the simulation environment."}),"\n",(0,t.jsx)(n.li,{children:"Start all the nodes."}),"\n",(0,t.jsx)(n.li,{children:"Clearly record yourself giving the voice command."}),"\n",(0,t.jsx)(n.li,{children:"Show the robot executing the full sequence: looking for the object, walking to it, and grasping it."}),"\n",(0,t.jsx)(n.li,{children:'Show the terminal output of your key nodes to illustrate what the robot is "thinking."'}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Conclusion (15s)"}),": Briefly summarize your success and what you learned."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"grading-rubric",children:"Grading Rubric"}),"\n",(0,t.jsx)(n.p,{children:"Your project will be evaluated based on the following criteria."}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Category"}),(0,t.jsx)(n.th,{children:"Weight"}),(0,t.jsx)(n.th,{children:"Description"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Functionality"})}),(0,t.jsx)(n.td,{children:"40%"}),(0,t.jsx)(n.td,{children:"The robot successfully completes the entire task from voice command to grasp. The system is robust."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Code Quality"})}),(0,t.jsx)(n.td,{children:"20%"}),(0,t.jsx)(n.td,{children:"Code is clean, well-commented, and follows ROS 2 best practices. The repository is properly structured."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"VLA Implementation"})}),(0,t.jsx)(n.td,{children:"20%"}),(0,t.jsx)(n.td,{children:"The integration of Speech-to-Text, LLM, Perception, Navigation, and Manipulation is seamless and well-architected."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Video Demo"})}),(0,t.jsx)(n.td,{children:"15%"}),(0,t.jsx)(n.td,{children:"The demo video is clear, concise, and effectively showcases the project's functionality and your understanding."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Documentation"})}),(0,t.jsx)(n.td,{children:"5%"}),(0,t.jsxs)(n.td,{children:["The ",(0,t.jsx)(n.code,{children:"README.md"})," file in your workspace clearly explains how to set up and run your project."]})]})]})]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(n.p,{children:"Completing this capstone project is a significant achievement. You have not only mastered the individual components of robotics and AI but have successfully woven them together into a coherent, intelligent system capable of operating in a physical (simulated) world. This project is a microcosm of the future of humanoid robotics and serves as a launching pad for your future explorations in the exciting field of Physical AI. Good luck!"}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"/docs/module-4/cognitive-planning-llm-to-ros2",children:(0,t.jsx)(n.strong,{children:"\u2190 Previous: Cognitive Planning with LLMs"})})," | ",(0,t.jsx)(n.a,{href:"/docs/weekly-breakdown",children:(0,t.jsx)(n.strong,{children:"Next: Weekly Breakdown \u2192"})})]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>r});var i=o(6540);const t={},a=i.createContext(t);function s(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);